{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmnist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.070612\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 11.6%\n",
      "Minibatch loss at step 500: 2.746874\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 1000: 1.695766\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1500: 1.156690\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2000: 0.890852\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2500: 1.101712\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 0.929452\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06a2697a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 676.918945\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 34.5%\n",
      "Minibatch loss at step 500: 191.075775\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 1000: 115.152420\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1500: 68.751595\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 41.438869\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2500: 25.365982\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3000: 15.501988\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.4%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0674049d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 350.717072\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 31.6%\n",
      "Minibatch loss at step 2: 991.033081\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 35.6%\n",
      "Minibatch loss at step 4: 131.571136\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 63.8%\n",
      "Minibatch loss at step 6: 5.051969\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 8: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Test accuracy: 73.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 496.412170\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 24.5%\n",
      "Minibatch loss at step 2: 1197.339600\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 35.1%\n",
      "Minibatch loss at step 4: 254.231567\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 55.0%\n",
      "Minibatch loss at step 6: 31.147463\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 64.3%\n",
      "Minibatch loss at step 8: 4.826458\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 10: 1.458974\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 12: 0.177388\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 14: 5.933673\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 16: 0.064656\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 18: 1.029908\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 20: 0.050763\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 22: 2.610489\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 34: 1.520174\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 38: 0.450435\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 46: 0.659599\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 48: 0.241871\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 54: 0.338554\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 56: 0.473867\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 58: 0.581054\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 68: 0.357154\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 76: 0.458496\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 78: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 84: 0.289129\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 88: 0.876840\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 90: 0.255620\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 96: 1.744245\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 98: 1.147553\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Test accuracy: 73.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.304368\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 41.9%\n",
      "Minibatch loss at step 500: 0.995503\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1000: 0.788214\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 1500: 0.649890\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2000: 0.617185\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2500: 0.740803\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3000: 0.598103\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3500: 0.421027\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4000: 0.481893\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4500: 0.546038\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5000: 0.480425\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5500: 0.576422\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6000: 0.474207\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.621828\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.378894\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 7500: 0.445192\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8000: 0.377851\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8500: 0.459451\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9000: 0.373565\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Test accuracy: 95.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.434265\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 36.0%\n",
      "Minibatch loss at step 500: 0.408008\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1000: 0.430338\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 1500: 0.318350\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2000: 0.351684\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2500: 0.525899\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 3000: 0.337600\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 3500: 0.188376\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 4000: 0.258575\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4500: 0.326133\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 5000: 0.271559\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5500: 0.290207\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 6000: 0.220132\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.392917\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7000: 0.156605\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7500: 0.173950\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8000: 0.132661\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8500: 0.169105\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 9000: 0.139193\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 9500: 0.070855\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 10000: 0.057698\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 10500: 0.102940\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 11000: 0.123238\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 11500: 0.078504\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 12000: 0.113322\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 12500: 0.048189\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 13000: 0.153169\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 13500: 0.016766\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14000: 0.120575\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14500: 0.121016\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 15000: 0.045929\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 15500: 0.103897\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 16000: 0.112826\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 16500: 0.011492\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 17000: 0.070595\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 17500: 0.022988\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 18000: 0.092392\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.3%\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.954450\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 17.1%\n",
      "Minibatch loss at step 500: 0.615891\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000: 0.527658\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1500: 0.483928\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2000: 0.633415\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2500: 0.693189\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 3000: 0.521652\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3500: 0.473346\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 4000: 0.514465\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 4500: 0.593401\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 5000: 0.436380\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 5500: 0.621870\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 6000: 0.473987\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6500: 0.675469\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 7000: 0.343021\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 7500: 0.467736\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 8000: 0.379106\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 8500: 0.426110\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9000: 0.394908\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 9500: 0.301676\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 10000: 0.253248\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10500: 0.322503\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 11000: 0.483054\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 11500: 0.321022\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 12000: 0.413293\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 12500: 0.353122\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 13000: 0.372484\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 13500: 0.377159\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 14000: 0.461656\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 14500: 0.239675\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 15000: 0.387465\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 15500: 0.669776\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 16000: 0.487592\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 16500: 0.193202\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17000: 0.398283\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17500: 0.256989\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18000: 0.433718\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 18500: 0.378062\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 19000: 0.373051\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19500: 0.336108\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 20000: 0.203849\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
